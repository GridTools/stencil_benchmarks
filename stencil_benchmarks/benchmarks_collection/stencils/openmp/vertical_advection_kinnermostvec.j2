{% extends "vertical_advection_kinnermost.j2" %}

{% block pre_kernel %}
{{ super() }}

using unaligned_vec_t = {{ ctype }} __attribute__((vector_size({{ vector_size }} * sizeof({{ ctype }})), aligned(sizeof({{ ctype }}))));
{%- if ctype == 'double' %}
    {%- if alignment == 8 * vector_size %}
    using vec_t = {{ ctype }} __attribute__((vector_size({{ vector_size }} * sizeof({{ ctype }}))));
    {%- else %}
    using vec_t = unaligned_vec_t;
    {%- endif %}
{%- else %}
    {%- if alignment == 4 * vector_size %}
    using vec_t = {{ ctype }} __attribute__((vector_size({{ vector_size }} * sizeof({{ ctype }}))));
    {%- else %}
    using vec_t = unaligned_vec_t;
    {%- endif %}
{%- endif %}

__attribute__((always_inline)) inline void forward_sweep_vec(index_t i,
    const index_t j,
    const index_t ishift,
    const index_t jshift,
    {{ ctype }} *__restrict__ ccol,
    {{ ctype }} *__restrict__ dcol,
    const {{ ctype }} *__restrict__ wcon,
    const {{ ctype }} *__restrict__ ustage,
    const {{ ctype }} *__restrict__ upos,
    const {{ ctype }} *__restrict__ utens,
    const {{ ctype }} *__restrict__ utensstage) {

    vec_t ccol0, ccol1;
    vec_t dcol0, dcol1;
    vec_t ustage0, ustage1, ustage2;
    vec_t wcon0, wcon1;
    vec_t wcon_shift0, wcon_shift1;

    index_t index = i * {{ strides[0] }} + j * {{ strides[1] }};
    // k minimum
    {
        wcon_shift0 = *(unaligned_vec_t*)&wcon[index + ishift * {{ strides[0] }} + jshift * {{ strides[1] }} + {{ strides[2] }}];
        wcon0 = *(vec_t*)&wcon[index + {{ strides[2] }}];
        vec_t gcv = {{ ctype }}(0.25) * (wcon_shift0 + wcon0);
        vec_t cs = gcv * bet_m;

        ccol0 = gcv * bet_p;
        vec_t bcol = dtr_stage - ccol0;

        ustage0 = *(vec_t*)&ustage[index + {{ strides[2] }}];
        ustage1 = *(vec_t*)&ustage[index];
        vec_t correction_term = -cs * (ustage0 - ustage1);
        dcol0 = dtr_stage * *(vec_t*)&upos[index] + *(vec_t*)&utens[index] + *(vec_t*)&utensstage[index] + correction_term;

        vec_t divided = {{ ctype }}(1.0) / bcol;
        ccol0 = ccol0 * divided;
        dcol0 = dcol0 * divided;

        *(vec_t*)&ccol[index] = ccol0;
        *(vec_t*)&dcol[index] = dcol0;

        index += {{ strides[2] }};
    }

    // k body
    for (index_t k = 1; k < {{ domain[2] }} - 1; ++k) {
        {% if prefetch_distance > 0 %}
#ifdef __SSE__
        constexpr index_t prefdist = {{ prefetch_distance }};
        if (k < {{ domain[2] }} - prefdist) {
            const index_t prefindex = index + prefdist * {{ strides[2] }};
            _mm_prefetch(reinterpret_cast<const char *>(&upos[prefindex]), _MM_HINT_T1);
            _mm_prefetch(reinterpret_cast<const char *>(&ustage[prefindex + {{ strides[2] }}]), _MM_HINT_T1);
            _mm_prefetch(reinterpret_cast<const char *>(&utens[prefindex]), _MM_HINT_T1);
            _mm_prefetch(reinterpret_cast<const char *>(&utensstage[prefindex]), _MM_HINT_T1);
            _mm_prefetch(reinterpret_cast<const char *>(&wcon[prefindex + {{ strides[2] }}]), _MM_HINT_T1);
            _mm_prefetch(reinterpret_cast<const char *>(
                                &wcon[prefindex + ishift * {{ strides[0] }} + jshift * {{ strides[1] }} + {{ strides[2] }}]),
                _MM_HINT_T1);
            _mm_prefetch(reinterpret_cast<const char *>(&ccol[prefindex]), _MM_HINT_T1);
            _mm_prefetch(reinterpret_cast<const char *>(&dcol[prefindex]), _MM_HINT_T1);
        }
#else
        constexpr index_t prefdist = {{ prefetch_distance }};
        if (k < {{ domain[2] }} - prefdist) {
            const index_t prefindex = index + prefdist * {{ strides[2] }};
            __builtin_prefetch(reinterpret_cast<const char *>(&upos[prefindex]));
            __builtin_prefetch(reinterpret_cast<const char *>(&ustage[prefindex + {{ strides[2] }}]));
            __builtin_prefetch(reinterpret_cast<const char *>(&utens[prefindex]));
            __builtin_prefetch(reinterpret_cast<const char *>(&utensstage[prefindex]));
            __builtin_prefetch(reinterpret_cast<const char *>(&wcon[prefindex + {{ strides[2] }}]));
            __builtin_prefetch(reinterpret_cast<const char *>(
                                &wcon[prefindex + ishift * {{ strides[0] }} + jshift * {{ strides[1] }} + {{ strides[2] }}]));
            __builtin_prefetch(reinterpret_cast<const char *>(&ccol[prefindex]));
            __builtin_prefetch(reinterpret_cast<const char *>(&dcol[prefindex]));
        }
#endif
        {% endif %}

        ccol1 = ccol0;
        dcol1 = dcol0;
        ustage2 = ustage1;
        ustage1 = ustage0;
        wcon1 = wcon0;
        wcon_shift1 = wcon_shift0;

        vec_t gav = {{ ctype }}(-0.25) * (wcon_shift1 + wcon1);
        wcon_shift0 = *(unaligned_vec_t*)&wcon[index + ishift * {{ strides[0] }} + jshift * {{ strides[1] }} + {{ strides[2] }}];
        wcon0 = *(vec_t*)&wcon[index + {{ strides[2] }}];
        vec_t gcv = {{ ctype }}(0.25) * (wcon_shift0 + wcon0);

        vec_t as = gav * bet_m;
        vec_t cs = gcv * bet_m;

        vec_t acol = gav * bet_p;
        ccol0 = gcv * bet_p;
        vec_t bcol = dtr_stage - acol - ccol0;

        ustage0 = *(vec_t*)&ustage[index + {{ strides[2] }}];
        vec_t correction_term = -as * (ustage2 - ustage1) - cs * (ustage0 - ustage1);
        dcol0 = dtr_stage * *(vec_t*)&upos[index] + *(vec_t*)&utens[index] + *(vec_t*)&utensstage[index] + correction_term;

        vec_t divided = {{ ctype }}(1.0) / (bcol - ccol1 * acol);
        ccol0 = ccol0 * divided;
        dcol0 = (dcol0 - dcol1 * acol) * divided;

        *(vec_t*)&ccol[index] = ccol0;
        *(vec_t*)&dcol[index] = dcol0;

        index += {{ strides[2] }};
    }

    // k maximum
    {
        ccol1 = ccol0;
        dcol1 = dcol0;
        ustage2 = ustage1;
        ustage1 = ustage0;
        wcon1 = wcon0;
        wcon_shift1 = wcon_shift0;

        vec_t gav = {{ ctype }}(-0.25) * (wcon_shift1 + wcon1);

        vec_t as = gav * bet_m;

        vec_t acol = gav * bet_p;
        vec_t bcol = dtr_stage - acol;

        vec_t correction_term = -as * (ustage2 - ustage1);
        dcol0 = dtr_stage * *(vec_t*)&upos[index] + *(vec_t*)&utens[index] + *(vec_t*)&utensstage[index] + correction_term;

        vec_t divided = {{ ctype }}(1.0) / (bcol - ccol1 * acol);
        dcol0 = (dcol0 - dcol1 * acol) * divided;

        *(vec_t*)&ccol[index] = ccol0;
        *(vec_t*)&dcol[index] = dcol0;
    }
}


__attribute__((always_inline)) inline void backward_sweep_vec(index_t i,
    const index_t j,
    const {{ ctype }} *__restrict__ ccol,
    const {{ ctype }} *__restrict__ dcol,
    const {{ ctype }} *__restrict__ upos,
    {{ ctype }} *__restrict__ utensstage) {
    constexpr {{ ctype }} dtr_stage = 3.0 / 20.0;

    vec_t datacol;

    index_t index = i * {{ strides[0] }} + j * {{ strides[1] }} + ({{ domain[2] }} - 1) * {{ strides[2] }};
    // k
    {
        datacol = *(vec_t*)&dcol[index];
        *(vec_t*)&utensstage[index] = dtr_stage * (datacol - *(vec_t*)&upos[index]);

        index -= {{ strides[2] }};
    }

    // k body
    for (index_t k = {{ domain[2] }} - 2; k >= 0; --k) {
        datacol = *(vec_t*)&dcol[index] - *(vec_t*)&ccol[index] * datacol;
        *(vec_t*)&utensstage[index] = dtr_stage * (datacol - *(vec_t*)&upos[index]);

        index -= {{ strides[2] }};
    }
}
{% endblock pre_kernel %}

{% block kernel_invoke %}
#pragma omp parallel for collapse(2)
    for (index_t jb = 0; jb < {{ domain[1] }}; jb += {{ block_size[1] }}) {
        for (index_t ib = 0; ib < {{ domain[0] }}; ib += {{ block_size[0] }}) {
            {%- if block_size[1] > 1 %}
                const index_t jmax = std::min((index_t){{ domain[1] }}, jb + {{ block_size[1] }});
                for (index_t j = jb; j < jmax; ++j) {
            {%- else %}
                const index_t j = jb;
                {
            {%- endif %}
                    const index_t imax = std::min((index_t){{ domain[0] }}, ib + {{ block_size[0] }});

                    index_t i;
                    for (i = ib; i < imax - {{ vector_size - 1}}; i += {{ vector_size }}) {
                        forward_sweep_vec(i, j, 1, 0, ccol, dcol, wcon, ustage, upos, utens, utensstage);
                        backward_sweep_vec(i, j, ccol, dcol, upos, utensstage);

                        forward_sweep_vec(i, j, 0, 1, ccol, dcol, wcon, vstage, vpos, vtens, vtensstage);
                        backward_sweep_vec(i, j, ccol, dcol, vpos, vtensstage);

                        forward_sweep_vec(i, j, 0, 0, ccol, dcol, wcon, wstage, wpos, wtens, wtensstage);
                        backward_sweep_vec(i, j, ccol, dcol, wpos, wtensstage);
                    }
                    for (; i < imax; ++i) {
                        forward_sweep(i, j, 1, 0, ccol, dcol, wcon, ustage, upos, utens, utensstage);
                        backward_sweep(i, j, ccol, dcol, upos, utensstage);

                        forward_sweep(i, j, 0, 1, ccol, dcol, wcon, vstage, vpos, vtens, vtensstage);
                        backward_sweep(i, j, ccol, dcol, vpos, vtensstage);

                        forward_sweep(i, j, 0, 0, ccol, dcol, wcon, wstage, wpos, wtens, wtensstage);
                        backward_sweep(i, j, ccol, dcol, wpos, wtensstage);
                    }
                }
        }
    }
{% endblock kernel_invoke %}
