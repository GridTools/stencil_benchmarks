{% extends "vertical_advection_kinnermost.j2" %}

{% block pre_kernel %}
{{ super() }}

__attribute__((always_inline)) inline void forward_sweep_vec(index_t i,
    const index_t j,
    const index_t ishift,
    const index_t jshift,
    vec_t *__restrict__ ccol,
    vec_t *__restrict__ dcol,
    const {{ ctype }} *__restrict__ wcon,
    const {{ ctype }} *__restrict__ ustage,
    const {{ ctype }} *__restrict__ upos,
    const {{ ctype }} *__restrict__ utens,
    const {{ ctype }} *__restrict__ utensstage) {

    vec_t ccol0, ccol1;
    vec_t dcol0, dcol1;
    vec_t ustage0, ustage1, ustage2;
    vec_t wcon0, wcon1;
    vec_t wcon_shift0, wcon_shift1;

    index_t index = i * {{ strides[0] }} + j * {{ strides[1] }};
    // k minimum
    {
        wcon_shift0 = *(unaligned_vec_t*)&wcon[index + ishift * {{ strides[0] }} + jshift * {{ strides[1] }} + {{ strides[2] }}];
        wcon0 = *(vec_t*)&wcon[index + {{ strides[2] }}];
        vec_t gcv = {{ ctype }}(0.25) * (wcon_shift0 + wcon0);
        vec_t cs = gcv * bet_m;

        ccol0 = gcv * bet_p;
        vec_t bcol = dtr_stage - ccol0;

        ustage0 = *(vec_t*)&ustage[index + {{ strides[2] }}];
        ustage1 = *(vec_t*)&ustage[index];
        vec_t correction_term = -cs * (ustage0 - ustage1);
        dcol0 = dtr_stage * *(vec_t*)&upos[index] + *(vec_t*)&utens[index] + *(vec_t*)&utensstage[index] + correction_term;

        vec_t divided = {{ ctype }}(1.0) / bcol;
        ccol0 = ccol0 * divided;
        dcol0 = dcol0 * divided;

        ccol[0] = ccol0;
        dcol[0] = dcol0;

        index += {{ strides[2] }};
    }

    // k body
    for (index_t k = 1; k < {{ domain[2] }} - 1; ++k) {
        {% if prefetch_distance > 0 %}
#ifdef __SSE__
        constexpr index_t prefdist = {{ prefetch_distance }};
        if (k < {{ domain[2] }} - prefdist) {
            const index_t prefindex = index + prefdist * {{ strides[2] }};
            _mm_prefetch(reinterpret_cast<const char *>(&upos[prefindex]), _MM_HINT_T1);
            _mm_prefetch(reinterpret_cast<const char *>(&ustage[prefindex + {{ strides[2] }}]), _MM_HINT_T1);
            _mm_prefetch(reinterpret_cast<const char *>(&utens[prefindex]), _MM_HINT_T1);
            _mm_prefetch(reinterpret_cast<const char *>(&utensstage[prefindex]), _MM_HINT_T1);
            _mm_prefetch(reinterpret_cast<const char *>(&wcon[prefindex + {{ strides[2] }}]), _MM_HINT_T1);
            _mm_prefetch(reinterpret_cast<const char *>(
                                &wcon[prefindex + ishift * {{ strides[0] }} + jshift * {{ strides[1] }} + {{ strides[2] }}]),
                _MM_HINT_T1);
        }
#else
        constexpr index_t prefdist = {{ prefetch_distance }};
        if (k < {{ domain[2] }} - prefdist) {
            const index_t prefindex = index + prefdist * {{ strides[2] }};
            __builtin_prefetch(reinterpret_cast<const char *>(&upos[prefindex]));
            __builtin_prefetch(reinterpret_cast<const char *>(&ustage[prefindex + {{ strides[2] }}]));
            __builtin_prefetch(reinterpret_cast<const char *>(&utens[prefindex]));
            __builtin_prefetch(reinterpret_cast<const char *>(&utensstage[prefindex]));
            __builtin_prefetch(reinterpret_cast<const char *>(&wcon[prefindex + {{ strides[2] }}]));
            __builtin_prefetch(reinterpret_cast<const char *>(
                                &wcon[prefindex + ishift * {{ strides[0] }} + jshift * {{ strides[1] }} + {{ strides[2] }}]));
        }
#endif
        {% endif %}

        ccol1 = ccol0;
        dcol1 = dcol0;
        ustage2 = ustage1;
        ustage1 = ustage0;
        wcon1 = wcon0;
        wcon_shift1 = wcon_shift0;

        vec_t gav = {{ ctype }}(-0.25) * (wcon_shift1 + wcon1);
        wcon_shift0 = *(unaligned_vec_t*)&wcon[index + ishift * {{ strides[0] }} + jshift * {{ strides[1] }} + {{ strides[2] }}];
        wcon0 = *(vec_t*)&wcon[index + {{ strides[2] }}];
        vec_t gcv = {{ ctype }}(0.25) * (wcon_shift0 + wcon0);

        vec_t as = gav * bet_m;
        vec_t cs = gcv * bet_m;

        vec_t acol = gav * bet_p;
        ccol0 = gcv * bet_p;
        vec_t bcol = dtr_stage - acol - ccol0;

        ustage0 = *(vec_t*)&ustage[index + {{ strides[2] }}];
        vec_t correction_term = -as * (ustage2 - ustage1) - cs * (ustage0 - ustage1);
        dcol0 = dtr_stage * *(vec_t*)&upos[index] + *(vec_t*)&utens[index] + *(vec_t*)&utensstage[index] + correction_term;

        vec_t divided = {{ ctype }}(1.0) / (bcol - ccol1 * acol);
        ccol0 = ccol0 * divided;
        dcol0 = (dcol0 - dcol1 * acol) * divided;

        ccol[k] = ccol0;
        dcol[k] = dcol0;

        index += {{ strides[2] }};
    }

    // k maximum
    {
        ccol1 = ccol0;
        dcol1 = dcol0;
        ustage2 = ustage1;
        ustage1 = ustage0;
        wcon1 = wcon0;
        wcon_shift1 = wcon_shift0;

        vec_t gav = {{ ctype }}(-0.25) * (wcon_shift1 + wcon1);

        vec_t as = gav * bet_m;

        vec_t acol = gav * bet_p;
        vec_t bcol = dtr_stage - acol;

        vec_t correction_term = -as * (ustage2 - ustage1);
        dcol0 = dtr_stage * *(vec_t*)&upos[index] + *(vec_t*)&utens[index] + *(vec_t*)&utensstage[index] + correction_term;

        vec_t divided = {{ ctype }}(1.0) / (bcol - ccol1 * acol);
        dcol0 = (dcol0 - dcol1 * acol) * divided;

        ccol[{{ domain[2] - 1 }}] = ccol0;
        dcol[{{ domain[2] - 1 }}] = dcol0;
    }
}


__attribute__((always_inline)) inline void backward_sweep_vec(index_t i,
    const index_t j,
    const vec_t *__restrict__ ccol,
    const vec_t *__restrict__ dcol,
    const {{ ctype }} *__restrict__ upos,
    {{ ctype }} *__restrict__ utensstage) {
    constexpr {{ ctype }} dtr_stage = 3.0 / 20.0;

    vec_t datacol;

    index_t index = i * {{ strides[0] }} + j * {{ strides[1] }} + ({{ domain[2] }} - 1) * {{ strides[2] }};
    // k
    {
        datacol = dcol[{{ domain[2] - 1}}];
        *(vec_t*)&utensstage[index] = dtr_stage * (datacol - *(vec_t*)&upos[index]);

        index -= {{ strides[2] }};
    }

    // k body
    for (index_t k = {{ domain[2] }} - 2; k >= 0; --k) {
        datacol = dcol[k] - ccol[k] * datacol;
        *(vec_t*)&utensstage[index] = dtr_stage * (datacol - *(vec_t*)&upos[index]);

        index -= {{ strides[2] }};
    }
}
{% endblock pre_kernel %}

{% block kernel_invoke %}
#pragma omp parallel
    {
        alignas({{ alignment }}) {{ ctype }} ccolb[{{ domain[2] * vector_size }}];
        alignas({{ alignment }}) {{ ctype }} dcolb[{{ domain[2] * vector_size }}];

#pragma omp for collapse(2)
        for (index_t jb = 0; jb < {{ domain[1] }}; jb += {{ block_size[1] }}) {
            for (index_t ib = 0; ib < {{ domain[0] }}; ib += {{ block_size[0] }}) {
                {%- if block_size[1] > 1 %}
                    const index_t jmax = std::min((index_t){{ domain[1] }}, jb + {{ block_size[1] }});
                    for (index_t j = jb; j < jmax; ++j) {
                {%- else %}
                    const index_t j = jb;
                    {
                {%- endif %}
                        const index_t imax = std::min((index_t){{ domain[0] }}, ib + {{ block_size[0] }});

                        index_t i;
                        for (i = ib; i < imax - {{ vector_size - 1}}; i += {{ vector_size }}) {
                            forward_sweep_vec(i, j, 1, 0, (vec_t*)ccolb, (vec_t*)dcolb, wcon, ustage, upos, utens, utensstage);
                            backward_sweep_vec(i, j, (vec_t*)ccolb, (vec_t*)dcolb, upos, utensstage);

                            forward_sweep_vec(i, j, 0, 1, (vec_t*)ccolb, (vec_t*)dcolb, wcon, vstage, vpos, vtens, vtensstage);
                            backward_sweep_vec(i, j, (vec_t*)ccolb, (vec_t*)dcolb, vpos, vtensstage);

                            forward_sweep_vec(i, j, 0, 0, (vec_t*)ccolb, (vec_t*)dcolb, wcon, wstage, wpos, wtens, wtensstage);
                            backward_sweep_vec(i, j, (vec_t*)ccolb, (vec_t*)dcolb, wpos, wtensstage);
                        }
                        for (; i < imax; ++i) {
                            forward_sweep(i, j, 1, 0, ccolb, dcolb, wcon, ustage, upos, utens, utensstage);
                            backward_sweep(i, j, ccolb, dcolb, upos, utensstage);

                            forward_sweep(i, j, 0, 1, ccolb, dcolb, wcon, vstage, vpos, vtens, vtensstage);
                            backward_sweep(i, j, ccolb, dcolb, vpos, vtensstage);

                            forward_sweep(i, j, 0, 0, ccolb, dcolb, wcon, wstage, wpos, wtens, wtensstage);
                            backward_sweep(i, j, ccolb, dcolb, wpos, wtensstage);
                        }
                    }
            }
        }
    }
{% endblock kernel_invoke %}
