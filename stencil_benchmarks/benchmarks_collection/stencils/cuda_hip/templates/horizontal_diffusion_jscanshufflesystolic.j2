{% extends "base.j2" %}

{% block gpu_kernel_body %}
    const {{ index_type }} ib = {{ index_type }}(threadIdx.x);

    const {{ index_type }} i = {{ index_type }}(blockIdx.x) * {{ block_size[0] }} + ib;
    const {{ index_type }} j_base = {{ index_type }}(blockIdx.y) * {{ block_size[1] }};
    const {{ index_type }} k = {{ index_type }}(blockIdx.z) * {{ block_size[2] }} + threadIdx.z;
    if (k >= {{ domain[2] }})
        return;

    {{ ctype }} lap[{{ block_size[1] + 2 }}];
    {{ ctype }} flx[{{ block_size[1] + 2 }}];
    {{ ctype }} fly[{{ block_size[1] + 2 }}];

    const {{ index_type }} ib_max = (blockIdx.x + 1) * {{ block_size[0] }} <= {{ domain[0] }} ? {{ block_size[0] }} : {{ domain[0] }} - blockIdx.x * {{ block_size[0] }};
    const {{ index_type }} jb_max = (blockIdx.y + 1) * {{ block_size[1] }} <= {{ domain[1] }} ? {{ block_size[1] }} : {{ domain[1] }} - blockIdx.y * {{ block_size[1] }};

#ifdef __HIPCC__
#define SHFL_UP(x) __shfl_up((x), 1)
#else
#define SHFL_UP(x) __shfl_up_sync(0xffffffff, (x), 1)
#endif

    const {{ index_type }} base_index = i * {{ strides[0] }} + j_base * {{ strides[1] }} + k * {{ strides[2] }};
    
    {
        {{ index_type }} index = base_index - {{ 2 * strides[0] }} - {{ strides[1] }};
        {{ index_type }} cache_index = 0;
        {{ ctype }} inp_ijm1 = {{ ctype }}(0), inp_ij = {{ ctype }}(0);


        if (threadIdx.x < ib_max + 4) {
            inp_ijm1 = inp[index - {{ strides[1] }}];
            inp_ij = inp[index];
        }

#pragma unroll
        for ({{ index_type }} jb = -1; jb < {{ block_size[1] + 1 }}; ++jb) {
            if (jb < jb_max + 1) {
                {{ ctype }} inp_ijp1 = {{ ctype }}(0);
                if (threadIdx.x < ib_max + 4)
                    inp_ijp1 = inp[index + {{ strides[1] }}];

                {{ ctype }} sum = 0;
                sum += -inp_ij;
                sum = SHFL_UP(sum);
                sum += {{ ctype }}(4) * inp_ij - inp_ijm1 - inp_ijp1;
                sum = SHFL_UP(sum);
                sum += -inp_ij;
                lap[cache_index] = sum;

                inp_ijm1 = inp_ij;
                inp_ij = inp_ijp1;
            }

            index += {{ strides[1] }};
            ++cache_index;
        }
    }

    {
        constexpr {{ index_type }} i_offset = 3;
        {{ index_type }} index = base_index - i_offset * {{ strides[0] }};
        {{ index_type }} cache_index = 1;

#pragma unroll
        for ({{ index_type }} jb = 0; jb < {{ block_size[1] }}; ++jb) {
            if (jb < jb_max) {
                {{ ctype }} inp_ij = {{ ctype }}(0);
                
                if (threadIdx.x >= 2 && threadIdx.x < ib_max + 4)
                    inp_ij = inp[index];

                {{ ctype }} flx_sum = 0, limit_sum = 0;
                flx_sum += -lap[cache_index];
                limit_sum += -inp_ij;
                flx_sum = SHFL_UP(flx_sum);
                limit_sum = SHFL_UP(limit_sum);
                flx_sum += lap[cache_index];
                limit_sum += inp_ij;
                flx[cache_index] = flx_sum;
                if (flx[cache_index] * limit_sum > {{ ctype }}(0))
                    flx[cache_index] = {{ ctype }}(0);
            }

            index += {{ strides[1] }};
            ++cache_index;
        }
    }

    {
        constexpr {{ index_type }} i_offset = 4;
        {{ index_type }} index = base_index - i_offset * {{ strides[0] }} - {{ strides[1] }};
        {{ index_type }} cache_index = 0;
        {{ ctype }} inp_ij = {{ ctype }}(0);
        if (threadIdx.x >= 4 && threadIdx.x < ib_max + 4)
            inp_ij = inp[index];

#pragma unroll
        for ({{ index_type }} jb = -1; jb < {{ block_size[1] }}; ++jb) {
            if (jb < jb_max) {
                {{ ctype }} inp_ijp1 = {{ ctype }}(0);
                if (threadIdx.x >= 4 && threadIdx.x < ib_max + 4)
                    inp_ijp1 = inp[index + {{ strides[1] }}];

                {{ ctype }} fly_sum = 0;
                fly_sum += lap[cache_index + 1] - lap[cache_index];
                fly_sum = SHFL_UP(fly_sum);
                fly[cache_index] = fly_sum;
                if (fly[cache_index] * (inp_ijp1 - inp_ij) > {{ ctype }}(0))
                    fly[cache_index] = 0;

                inp_ij = inp_ijp1;
            }

            index += {{ strides[1] }};
            ++cache_index;
        }
    }

    {
        constexpr {{ index_type }} i_offset = 4;
        {{ index_type }} index = base_index - i_offset * {{ strides[0] }};
        {{ index_type }} cache_index = 1;

#pragma unroll
        for ({{ index_type }} jb = 0; jb < {{ block_size[1] }}; ++jb) {
            if (jb < jb_max) {
                {{ ctype }} sum = 0;
                sum += flx[cache_index];
                sum = SHFL_UP(sum);
                if (threadIdx.x >= 4 && threadIdx.x < ib_max + 4) {
                    sum = inp[index] - coeff[index] * (flx[cache_index] -
                                                    sum +
                                                    fly[cache_index] -
                                                    fly[cache_index - 1]);
                    out[index] = sum;
                }
            }

            index += {{ strides[1] }};
            ++cache_index;
        }
    }
{% endblock gpu_kernel_body %}

{% block kernel_prepare %}
    if ({{ block_size[0] + 4 }} != device_properties.warpSize) {
        std::cerr << "only supported block size along first dimension is "
                  << (device_properties.warpSize - 4) << std::endl;
        return 1;
    }
    block_size = dim3({{ block_size[0] + 4 }},
                      1,
                      {{ block_size[2] }});
    grid_size = dim3({{ (domain[0] + block_size[0] - 1) // block_size[0] }},
                     {{ (domain[1] + block_size[1] - 1) // block_size[1] }},
                     {{ (domain[2] + block_size[2] - 1) // block_size[2] }});
{% endblock kernel_prepare %}
